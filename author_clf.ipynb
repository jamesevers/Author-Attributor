{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesevers/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# encoding: utf-8\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import unidecode\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "\n",
    "import cPickle as pickle\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "model = models.Word2Vec.load_word2vec_format('models/ruscorpora_russe.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##for single files\n",
    "def get_lines(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip(\"\\n\"))\n",
    "    return lines\n",
    "\n",
    "    \n",
    "def get_author(dirname):\n",
    "    lines = list()\n",
    "    for fname in os.listdir(dirname):\n",
    "        for line in open(os.path.join(dirname,fname)):\n",
    "            lines.append(line.lower().strip(\"\\n\"))\n",
    "    return lines\n",
    "\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "def stem_words(word): return stemmer.stem(word)\n",
    "\n",
    "def strip_decode(text): return text.decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_chapters(path):\n",
    "    '''\n",
    "    Breaks a text corpus into paragraphs and returns\n",
    "    the output as a list of string objects\n",
    "    '''\n",
    "    book = open(path).readlines()\n",
    "    chapters_d = defaultdict(list)\n",
    "    count = 0\n",
    "    for x in book:\n",
    "        if len(x) != 1:\n",
    "            chapters_d[count].append(x.replace(\"\\n\",\"\"))\n",
    "        else:\n",
    "            count += 1\n",
    "        \n",
    "        \n",
    "    for k,v in chapters_d.items():\n",
    "        if len(v) < 50:\n",
    "            del chapters_d[k]\n",
    "            \n",
    "    chapters = list()\n",
    "    for chap in chapters_d.values():\n",
    "        chapters.append(\" \".join(chap))\n",
    "        \n",
    "    return chapters\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### structure text in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_df(author, text):\n",
    "    s = {'text': pd.Series([x for x in text]),\n",
    "        'author': pd.Series([author for x in xrange(len(text))])}\n",
    "    return pd.DataFrame(s)\n",
    "\n",
    "def cat_dfs(df_list):\n",
    "    base = df_list[0]\n",
    "    for d in df_list[1:]:\n",
    "        base = base.append(d, ignore_index=True)\n",
    "    return base\n",
    "\n",
    "\n",
    "def cleaning_pipeline(df):\n",
    "    df = df.reset_index()\n",
    "    df = df[df['text'].map(len) > 5]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_words(text):\n",
    "    '''\n",
    "    Returns all tokens in input text,\n",
    "    removes special characters and punctuation\n",
    "    and returns words\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text.lower().strip())\n",
    "    tokens = [stem_words(x) for x in tokens]\n",
    "    words = [x.replace(\",\",\"\").replace(\".\",\"\").replace(\";\",\"\").replace(\":\",\"\")\n",
    "                for x in tokens]\n",
    "    \n",
    "    return words, tokens\n",
    "    \n",
    "\n",
    "\n",
    "def words_per_sent(text):\n",
    "    '''\n",
    "    Returns the number average word count\n",
    "    per sentence for a given input text\n",
    "    '''\n",
    "    words, tokens = token_words(text)\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    words_per_sentence = np.array([len(nltk.word_tokenize(s))\n",
    "                                       for s in sentences])\n",
    "    return words_per_sentence.mean()\n",
    "\n",
    "\n",
    "def lexical_div(text):\n",
    "    '''\n",
    "    Returns the ratio of unique words\n",
    "    to total words in a given text to\n",
    "    measure diversity in diction\n",
    "    '''\n",
    "    words, tokens = token_words(text)\n",
    "    vocab = set(words)\n",
    "    \n",
    "    return len(vocab) / float(len(words))\n",
    "\n",
    "\n",
    "def len_var(text):\n",
    "    '''\n",
    "    Returns the variance between lengths of each\n",
    "    sentence in the paragraph\n",
    "    '''\n",
    "    sentences = sent_tokenize(text)\n",
    "    words_per_sentence = np.array([len(nltk.word_tokenize(s))\n",
    "                                       for s in sentences])\n",
    "    return words_per_sentence.std()\n",
    "\n",
    "\n",
    "def punct_per_sent(text, punct):\n",
    "    '''\n",
    "    Returns the average punctuation count\n",
    "    per sentence in the paragraph\n",
    "    '''\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens, words = sent_tokenize(text)\n",
    "    return tokens.count(str(punct)) / float(len(sentences))\n",
    "\n",
    "\n",
    "def syntax_vector(text):\n",
    "    '''\n",
    "    Returns the distribution of select\n",
    "    parts of speech in the paragraph\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = [p[1] for p in nltk.pos_tag(tokens)]\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    tag_list = np.array([tags.count(pos) for pos in pos_list])\n",
    "    return tag_list.std()\n",
    "\n",
    "\n",
    "def get_word_vectors(text):\n",
    "    '''\n",
    "    Positions new words from paragraph\n",
    "    with vectors from trained model\n",
    "    '''\n",
    "    words, tokens = token_words(text)\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vecs.append(model[word].reshape((1,300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs, dtype='float')\n",
    "\n",
    "\n",
    "def vector_avg(auth):\n",
    "    '''\n",
    "    Returns the average of vector\n",
    "    coordinates for a target text\n",
    "    '''\n",
    "\n",
    "    vectors = get_word_vectors(auth)\n",
    "    vectors_sum = 0\n",
    "    count = 0\n",
    "    for v in vectors:\n",
    "        count += 1\n",
    "        vectors_sum = np.add(vectors_sum,v)\n",
    "\n",
    "    #calculate the average vector and replace +infy and -inf with numeric values \n",
    "    avg_vector = np.nan_to_num(vectors_sum/count)\n",
    "    return sum(avg_vector)**2\n",
    "\n",
    "\n",
    "\n",
    "def features_pipe(df):\n",
    "    df['text'] = df['text'].map(strip_decode)\n",
    "    df['wps'] = df['text'].map(words_per_sent)\n",
    "    df['lex_fv'] = df['text'].map(lexical_div)\n",
    "    df['len_var'] = df['text'].map(len_var)\n",
    "    df['syntax_fv'] = df['text'].map(syntax_vector)\n",
    "    df['word_vec'] = df['text'].map(vector_avg)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### append syntactic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auth_gen(**kwargs):\n",
    "    authors_d = dict()\n",
    "    for k,v in kwargs.items():\n",
    "        if k in authors_d:\n",
    "            authors_d[k].append(get_chapters('authors/'+k+'/'+v+\".txt\"))\n",
    "        authors_d[k] = get_chapters('authors/'+k+'/'+v+\".txt\")\n",
    "        \n",
    "    dfs = [make_df(k,v) for k,v in authors_d.items()]\n",
    "    df = cleaning_pipeline(cat_dfs(dfs))\n",
    "    return features_pipe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "auth_works = {'zamyatin':'we', 'turgenev':'fathers_sons'}\n",
    "df = auth_gen(**auth_works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xy(df):\n",
    "    y = df['author']\n",
    "    X = df\n",
    "    X.drop('author', axis=1,inplace=True)\n",
    "    X.drop('text', axis=1, inplace=True)\n",
    "    X.drop('index', axis=1, inplace=True)\n",
    "    X, y = shuffle(X,y, random_state=0)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y = get_xy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_predict(X, y):\n",
    "    '''\n",
    "    \n",
    "    second half of process uses ensemble methods with word vector features\n",
    "    and lexographic, syntactic features, returns class predictions\n",
    "    '''\n",
    "    \n",
    "    \n",
    "\n",
    "    clf1 = LogisticRegression()\n",
    "    clf2 = RandomForestClassifier()\n",
    "    clf3 = GaussianNB()\n",
    "    #clf3 = SVC()\n",
    "    \n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('mnb', clf3)],\n",
    "                            voting='soft', weights=[1,1,1])\n",
    "\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    \n",
    "    for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression',\n",
    "                                                     'Random Forest','naive Bayes', 'Ensemble']):\n",
    "        scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "        \n",
    "    \n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84 (+/- 0.14) [Logistic Regression]\n",
      "Accuracy: 0.85 (+/- 0.15) [Random Forest]\n",
      "Accuracy: 0.84 (+/- 0.14) [naive Bayes]\n",
      "Accuracy: 0.84 (+/- 0.20) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "train_predict(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
